# Plan: Implement CarbonCredits.com Scraping with Database Storage and 1-Minute Updates

## Context

The user wants to implement scraping for EUA prices from CarbonCredits.com (https://carboncredits.com/carbon-prices-today/), store historical prices in the database, and update prices every 1 minute. This is a new data source that should be added to the existing scraper system, and requires:

1. Adding CarbonCredits.com as a scraping source
2. Creating a database model to store historical price data
3. Implementing a background scheduler to scrape prices every 1 minute
4. Storing each scraped price in the database

## Current Implementation Analysis

**File**: `backend/scraper.py`

**Current Sources** (in priority order):
1. `_fetch_from_ice_spot()` - ICE Endex spot market data
2. `_fetch_from_alphavantage()` - Alpha Vantage API (if API key provided)
3. `_fetch_from_tradingview()` - TradingView web scraping
4. `_fetch_from_investing()` - Investing.com web scraping
5. `_fetch_from_marketwatch()` - MarketWatch web scraping
6. `_fetch_from_ice_public()` - ICE public pages

**Current Storage**:
- Historical prices are stored in JSON files (`backend/data/historical_eua.json`, `backend/data/historical_cer.json`)
- No database model exists for storing price history
- Prices are cached in memory for 2 minutes (`CACHE_DURATION = 120`)

**Current Update Frequency**:
- Prices are fetched on-demand when API endpoints are called
- No background scheduler exists for automatic updates

## Technical Requirements

### 1. Add CarbonCredits.com Scraping Method

**File**: `backend/scraper.py`

**Changes**:
- Add new method `_fetch_from_carboncredits()` to `ICEScraper` class
- URL: `https://carboncredits.com/carbon-prices-today/`
- Parse HTML to extract EUA price
- Return dictionary with same format as other sources:
  ```python
  {
      'price': float,  # EUA price in EUR
      'timestamp': datetime,  # UTC timezone-aware
      'currency': 'EUR',
      'change24h': float | None,  # Optional 24h change percentage
  }
  ```
- Add to `source_map` dictionary with key: `"CarbonCredits.com"`
- Add to `sources` list in `scrape_ice_price()` method (priority: after ICE spot, before Alpha Vantage)

**Implementation Details**:
- Use BeautifulSoup to parse HTML
- Look for EUA price in structured HTML (likely in tables or specific divs)
- Handle potential bot detection (use proper User-Agent headers)
- Validate price range (EUA typically 50-100 EUR)
- Handle errors gracefully (return None if scraping fails)

### 2. Create Database Model for Price History

**File**: `backend/models/price_history.py` (new file)

**Model**: `PriceHistory`

**Fields**:
- `id`: Integer primary key (auto-increment)
- `price`: Float (required) - EUA price in EUR
- `currency`: String(3) (required, default='EUR') - Currency code
- `source`: String(100) (required) - Data source name (e.g., "CarbonCredits.com", "ICE (Intercontinental Exchange)")
- `timestamp`: DateTime (required, indexed) - UTC timestamp when price was fetched
- `change24h`: Float (optional) - 24-hour price change percentage
- `created_at`: DateTime (required, default=now) - Record creation timestamp

**Indexes**:
- Index on `timestamp` for efficient date range queries
- Index on `source` for filtering by source
- Composite index on `(timestamp, source)` for efficient queries

**Methods**:
- `to_dict(camel_case=False)`: Serialize to dictionary for API responses

**File**: `backend/models/__init__.py`

**Changes**:
- Import `PriceHistory` model: `from .price_history import PriceHistory`
- Export in `__all__` list

### 3. Update Database Initialization

**File**: `backend/app.py`

**Changes**:
- Import `PriceHistory` model: `from models.price_history import PriceHistory`
- Ensure model is imported before `db.create_all()` is called (models are auto-discovered via blueprints, but explicit import ensures it's registered)

**File**: `backend/database.py`

**No changes needed** - SQLAlchemy will auto-discover models when imported.

### 4. Implement Background Scheduler

**Option A: APScheduler (Recommended)**

**Dependencies**:
- Add `APScheduler==3.10.4` to `backend/requirements.txt`

**File**: `backend/app.py`

**Changes**:
- Import APScheduler: `from apscheduler.schedulers.background import BackgroundScheduler`
- Import threading: `import threading`
- Create scheduler instance after app initialization:
  ```python
  scheduler = BackgroundScheduler()
  scheduler.start()
  ```
- Add scheduled job function:
  ```python
  def scheduled_price_update():
      """Background job to fetch and store EUA price every 1 minute"""
      with app.app_context():
          try:
              # Fetch price from scraper
              price_data = scraper.scrape_ice_price()
              
              # If scraping fails, try alternative sources
              if not price_data:
                  api_key = os.getenv('OILPRICE_API_KEY')
                  price_data = alternative_source.fetch_from_oilprice_api(api_key)
              
              # If still no data, skip this update (don't store None)
              if not price_data or not price_data.get('price'):
                  logger.warning("Scheduled price update: No price data available")
                  return
              
              # Store in database
              price_entry = PriceHistory(
                  price=price_data['price'],
                  currency=price_data.get('currency', 'EUR'),
                  source=price_data.get('source', 'Unknown'),
                  timestamp=price_data['timestamp'],
                  change24h=price_data.get('change24h')
              )
              db.session.add(price_entry)
              db.session.commit()
              
              # Update cache
              global cached_response, last_fetch_time
              cached_response = price_data
              last_fetch_time = datetime.now(timezone.utc)
              
              # Update source price history
              update_source_price_history(price_data)
              
              logger.info(f"Scheduled price update: Stored price â‚¬{price_data['price']} from {price_data.get('source', 'Unknown')}")
          except Exception as e:
              logger.error(f"Scheduled price update failed: {e}")
              db.session.rollback()
  ```
- Schedule job after scheduler start:
  ```python
  scheduler.add_job(
      func=scheduled_price_update,
      trigger='interval',
      minutes=1,
      id='eua_price_update',
      name='EUA Price Update (1 minute)',
      replace_existing=True
  )
  ```
- Add shutdown handler:
  ```python
  import atexit
  atexit.register(lambda: scheduler.shutdown())
  ```

**Option B: Threading.Timer (Alternative, Simpler)**

**File**: `backend/app.py`

**Changes**:
- Use `threading.Timer` for simpler implementation (no external dependency)
- Create recursive timer function that reschedules itself
- Less robust than APScheduler but simpler for basic use case

**Recommendation**: Use APScheduler for better error handling, job management, and production readiness.

### 5. Update Source Tracking

**File**: `backend/app.py`

**Changes**:
- Add constant: `SOURCE_CARBONCREDITS = 'CarbonCredits.com'`
- Add to `source_price_history` dictionary:
  ```python
  SOURCE_CARBONCREDITS: {'price': None, 'timestamp': None},
  ```
- Update `get_price_updates_status()` endpoint:
  - Add `SOURCE_CARBONCREDITS` to `dataSources` list
  - Include in `sourcePrices` array

**File**: `backend/scraper.py`

**Changes**:
- Update `source_map` dictionary to include CarbonCredits.com method mapping

### 6. Add API Endpoint for Historical Price Query

**File**: `backend/app.py`

**New Endpoint**: `GET /api/eua/price/history`

**Query Parameters**:
- `start_date` (optional): ISO date string (YYYY-MM-DD) or ISO datetime
- `end_date` (optional): ISO date string (YYYY-MM-DD) or ISO datetime
- `source` (optional): Filter by source name
- `limit` (optional): Maximum number of results (default: 1000)

**Response**:
```json
{
  "data": [
    {
      "id": 1,
      "price": 79.50,
      "currency": "EUR",
      "source": "CarbonCredits.com",
      "timestamp": "2024-01-01T12:00:00Z",
      "change24h": 0.5,
      "createdAt": "2024-01-01T12:00:01Z"
    },
    ...
  ],
  "count": 100,
  "startDate": "2024-01-01T00:00:00Z",
  "endDate": "2024-01-01T23:59:59Z"
}
```

**Implementation**:
- Query `PriceHistory` table filtered by date range
- Filter by source if provided
- Order by timestamp descending (newest first)
- Limit results if specified
- Return camelCase format for frontend compatibility

### 7. Migration Strategy

**Option A: Keep JSON Files (Hybrid Approach)**
- Continue using JSON files for historical data (pre-scheduler data)
- Database stores only new prices from scheduler (post-implementation)
- Historical endpoints can query both sources and merge results

**Option B: Migrate to Database Only**
- Create migration script to import existing JSON data into database
- Remove JSON file dependency for new data
- Keep JSON files as backup/archive

**Recommendation**: Start with Option A (hybrid) for backward compatibility, then migrate to Option B in future update.

## Implementation Steps

### Phase 1: Database Model and Scraping Method
1. Create `backend/models/price_history.py` with `PriceHistory` model
2. Update `backend/models/__init__.py` to export model
3. Add `_fetch_from_carboncredits()` method to `backend/scraper.py`
4. Update `source_map` and `sources` list in scraper
5. Test scraping method manually
6. Run database migration to create table

### Phase 2: Background Scheduler
1. Add `APScheduler==3.10.4` to `backend/requirements.txt`
2. Implement `scheduled_price_update()` function in `backend/app.py`
3. Initialize scheduler and add job
4. Test scheduler starts correctly
5. Verify prices are stored in database every minute
6. Add error handling and logging

### Phase 3: Source Tracking and API
1. Add `SOURCE_CARBONCREDITS` constant to `backend/app.py`
2. Update `source_price_history` dictionary
3. Update `get_price_updates_status()` endpoint
4. Create `GET /api/eua/price/history` endpoint
5. Test endpoint with various query parameters

### Phase 4: Testing and Validation
1. Test CarbonCredits.com scraping works correctly
2. Verify scheduler runs every 1 minute
3. Verify database entries are created
4. Test historical price query endpoint
5. Test source tracking displays CarbonCredits.com correctly
6. Verify error handling when scraping fails

## Files to Create

1. `backend/models/price_history.py` - Database model for price history

## Files to Modify

1. `backend/scraper.py`
   - Add `_fetch_from_carboncredits()` method
   - Update `source_map` dictionary
   - Update `sources` list in `scrape_ice_price()`

2. `backend/app.py`
   - Import `PriceHistory` model
   - Add `SOURCE_CARBONCREDITS` constant
   - Update `source_price_history` dictionary
   - Implement `scheduled_price_update()` function
   - Initialize APScheduler and add job
   - Add `GET /api/eua/price/history` endpoint
   - Update `get_price_updates_status()` endpoint

3. `backend/models/__init__.py`
   - Import and export `PriceHistory` model

4. `backend/requirements.txt`
   - Add `APScheduler==3.10.4`

5. `src/components/admin/PriceUpdateMonitoring.tsx` (optional)
   - Add "CarbonCredits.com" to default `dataSources` array if hardcoded

## Database Schema

**Table**: `price_history`

```sql
CREATE TABLE price_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    price FLOAT NOT NULL,
    currency VARCHAR(3) NOT NULL DEFAULT 'EUR',
    source VARCHAR(100) NOT NULL,
    timestamp DATETIME NOT NULL,
    change24h FLOAT,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_price_history_timestamp ON price_history(timestamp);
CREATE INDEX idx_price_history_source ON price_history(source);
CREATE INDEX idx_price_history_timestamp_source ON price_history(timestamp, source);
```

## Configuration

**Environment Variables** (optional, for future use):
- `PRICE_UPDATE_INTERVAL_MINUTES` (default: 1) - Configurable update interval
- `MAX_PRICE_HISTORY_DAYS` (default: 365) - Auto-cleanup old records after N days

## Error Handling

1. **Scraping Failures**: Log warning, skip database entry, continue scheduler
2. **Database Errors**: Log error, rollback transaction, continue scheduler
3. **Scheduler Errors**: Log error, scheduler continues running
4. **Network Errors**: Retry logic in scraper, fallback to cached price

## Performance Considerations

1. **Database Growth**: Price history will grow by ~1,440 entries per day (1 per minute)
   - ~525,600 entries per year
   - Consider periodic cleanup of old records (e.g., keep only last 2 years)
   - Consider archiving old data to separate table or file

2. **Query Performance**: 
   - Indexes on `timestamp` and `source` for efficient queries
   - Limit default query results to prevent large responses
   - Consider pagination for large date ranges

3. **Scheduler Overhead**:
   - 1-minute interval is reasonable for price updates
   - Scraping should complete in < 10 seconds
   - Database insert is fast (< 1ms)

## Testing Considerations

1. Test CarbonCredits.com scraping with real URL
2. Test scheduler runs every 1 minute (use shorter interval for testing)
3. Test database storage and retrieval
4. Test error handling when scraping fails
5. Test historical query endpoint with various parameters
6. Test source tracking displays correctly
7. Test concurrent scheduler and API requests

## Notes

- CarbonCredits.com is recommended as easy to scrape (HTML structured, no aggressive bot blocking)
- Database storage enables better querying and analysis than JSON files
- 1-minute updates provide near real-time price tracking
- Consider adding data retention policy to prevent database bloat
- Consider adding admin endpoint to manually trigger price update
- Consider adding monitoring/alerting if scheduler stops working

