# Plan: Replace Simulated Prices with Real Live EU ETS Price Data

## Context

Currently, the system uses a simulated price source (`_fetch_from_free_api()`) that generates "realistic" prices using random number generation and mathematical patterns. The user wants to replace all simulated price sources with real, live price data from actual market data providers.

## Current Implementation Analysis

**File**: `backend/scraper.py`

**Current Sources** (in priority order):
1. `_fetch_from_ice_spot()` - Attempts web scraping from ICE website
2. `_fetch_from_free_api()` - **SIMULATED** - Uses random/math to generate prices
3. `_fetch_from_tradingview()` - Web scraping from TradingView
4. `_fetch_from_investing()` - Web scraping from Investing.com
5. `_fetch_from_marketwatch()` - Web scraping from MarketWatch
6. `_fetch_from_ice_public()` - Web scraping from ICE public pages
7. `fetch_from_oilprice_api()` - API fallback (may not have EU ETS data)

**Issues**:
- `_fetch_from_free_api()` generates fake prices using `random` and `math` modules
- Web scraping sources are unreliable and may fail frequently
- No real API integration for guaranteed live data

## Technical Requirements

### 1. Remove Simulated Price Source

**File**: `backend/scraper.py`

**Changes**:
- Remove `_fetch_from_free_api()` method entirely
- Remove it from `source_map` dictionary
- Remove it from `sources` list in `scrape_ice_price()`
- Update source name constants in `backend/app.py` to remove "Free API (realistic simulation)"

### 2. Integrate Real API Providers

**Priority Order for Real APIs**:

#### Option A: TradingView API (Recommended - Free Tier Available)
- **API**: TradingView Charting Library or Symbol Info API
- **Endpoint**: `https://symbol-search.tradingview.com/symbol_search/` or use TradingView's public data endpoints
- **Implementation**: Create `_fetch_from_tradingview_api()` method
- **Authentication**: May require API key for reliable access
- **Rate Limits**: Check TradingView's terms of service

#### Option B: Alpha Vantage API (Free Tier Available)
- **API**: Alpha Vantage Market Data API
- **Endpoint**: `https://www.alphavantage.co/query`
- **Parameters**: Function=GLOBAL_QUOTE, symbol=EUA (if available)
- **Implementation**: Create `_fetch_from_alphavantage()` method
- **Authentication**: Requires free API key (500 calls/day limit)
- **Note**: May not have EU ETS specifically, need to verify

#### Option C: Polygon.io (Paid, but Reliable)
- **API**: Polygon.io Market Data API
- **Endpoint**: `https://api.polygon.io/v2/aggs/ticker/{ticker}/prev`
- **Implementation**: Create `_fetch_from_polygon()` method
- **Authentication**: Requires API key (paid service)
- **Note**: Check if they have EU ETS carbon market data

#### Option D: EEX (European Energy Exchange) - Official Source
- **API**: EEX Market Data API (may require commercial access)
- **Endpoint**: Check EEX documentation for EUA spot/futures data
- **Implementation**: Create `_fetch_from_eex()` method
- **Authentication**: Likely requires commercial API access
- **Note**: This is the official exchange, most reliable but may require payment

#### Option E: CarbonCredits.com API (If Available)
- **API**: Check if CarbonCredits.com offers API access
- **Implementation**: Create `_fetch_from_carboncredits()` method
- **Authentication**: Check their API documentation

### 3. Improve Web Scraping Reliability

**Files**: `backend/scraper.py`

**Changes**:
- Keep existing web scraping methods as fallbacks
- Improve error handling and retry logic
- Add more robust HTML parsing
- Add request rate limiting to avoid being blocked
- Consider using Selenium/Playwright for JavaScript-rendered content if needed

### 4. Environment Variables for API Keys

**File**: `backend/app.py` and `docker-compose.yml`

**New Environment Variables**:
- `TRADINGVIEW_API_KEY` (optional)
- `ALPHAVANTAGE_API_KEY` (optional)
- `POLYGON_API_KEY` (optional)
- `EEX_API_KEY` (optional)
- `CARBONCREDITS_API_KEY` (optional)

**Configuration**:
- Add to `docker-compose.yml` environment section
- Add to `.env.example` if exists
- Document in `backend/README.md`

### 5. Update Source Tracking

**File**: `backend/app.py`

**Changes**:
- Remove `SOURCE_FREE_API` constant
- Update `source_price_history` dictionary to remove "Free API (realistic simulation)"
- Update `get_price_updates_status()` to remove from `sourcePrices` array
- Update `dataSources` list in status response

**File**: `src/components/admin/PriceUpdateMonitoring.tsx`

**Changes**:
- Remove "Free API (realistic simulation)" from default `dataSources` array (line 41)

### 6. Error Handling and Fallbacks

**Strategy**:
1. Try real API sources first (in order of reliability)
2. Fall back to improved web scraping
3. Fall back to cached price if all sources fail
4. Return error only if no cached price available

**Implementation**:
- Update `scrape_ice_price()` method to prioritize real APIs
- Add comprehensive error logging for each source failure
- Ensure at least one reliable source is configured

## Implementation Steps

### Phase 1: Remove Simulation
1. Remove `_fetch_from_free_api()` method from `scraper.py`
2. Remove from source_map and sources list
3. Update source constants in `app.py`
4. Update frontend default data sources list
5. Test that system still works with remaining sources

### Phase 2: Integrate Real APIs
1. Research and select primary API provider (recommend starting with TradingView or Alpha Vantage)
2. Implement `_fetch_from_tradingview_api()` or `_fetch_from_alphavantage()` method
3. Add API key configuration to environment variables
4. Update source_map and sources list with new method
5. Test API integration with real API key
6. Add error handling for API failures

### Phase 3: Add Additional APIs (Optional)
1. Implement secondary API provider as backup
2. Add to sources list in priority order
3. Test fallback chain

### Phase 4: Improve Web Scraping (Optional)
1. Enhance existing web scraping methods
2. Add retry logic and better error handling
3. Consider Selenium for JavaScript-heavy sites if needed

## Files to Modify

1. `backend/scraper.py`
   - Remove `_fetch_from_free_api()` method
   - Add new API methods (e.g., `_fetch_from_tradingview_api()`, `_fetch_from_alphavantage()`)
   - Update `scrape_ice_price()` source list
   - Update source_map dictionary

2. `backend/app.py`
   - Remove `SOURCE_FREE_API` constant
   - Update `source_price_history` dictionary
   - Update `get_price_updates_status()` response

3. `src/components/admin/PriceUpdateMonitoring.tsx`
   - Remove "Free API (realistic simulation)" from default dataSources array

4. `docker-compose.yml`
   - Add new API key environment variables

5. `backend/README.md`
   - Document new API key requirements
   - Update API endpoint documentation

6. `.env.example` (if exists)
   - Add new API key variables

## API Provider Recommendations

**For Free/Development Use**:
1. **TradingView Symbol Info** - Free, may have rate limits
2. **Alpha Vantage** - Free tier: 500 calls/day, requires API key
3. **Web Scraping** - Free but unreliable, keep as fallback

**For Production Use**:
1. **Polygon.io** - Paid but reliable, check EU ETS coverage
2. **EEX API** - Official exchange, likely requires commercial access
3. **ICE Data Services** - Official exchange API, commercial access required

## Testing Considerations

1. Test with API keys configured
2. Test without API keys (should fall back to web scraping)
3. Test API failure scenarios (invalid key, rate limit, network error)
4. Test fallback chain (API → Web Scraping → Cached)
5. Verify source tracking works correctly with new sources
6. Test that admin monitoring page displays correct source information

## Notes

- Start with one reliable API provider first (recommend TradingView or Alpha Vantage)
- Keep web scraping as fallback for when APIs are unavailable
- Ensure proper error handling so system doesn't break if APIs fail
- Consider rate limiting to avoid hitting API limits
- Document API key requirements clearly for deployment

